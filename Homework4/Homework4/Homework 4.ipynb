{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd266469",
   "metadata": {},
   "source": [
    "### Assignment 1\n",
    "\n",
    "Finalize Exercise 11.1 in `Week6/11_Full_MatSci-ML_Pipeline.ipynb`. Please, fill in the details and results in this [Google Sheet](https://docs.google.com/spreadsheets/d/1xbT4lRMYQGrhBQGFczFD5wscrriwWQGf82Gv9AnDkbA/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a656ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matminer.datasets.dataset_retrieval import load_dataset, get_all_dataset_info\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pymatgen.core import Composition\n",
    "from matminer.featurizers.composition.composite import ElementProperty, ElementFraction\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print(get_all_dataset_info('citrine_thermal_conductivity'))\n",
    "\n",
    "# dataset analysis\n",
    "\n",
    "df = load_dataset('citrine_thermal_conductivity')\n",
    "df.describe()\n",
    "\n",
    "# Remove the highest thermal conductivity data point (potential outlier)\n",
    "max_k = df['k_expt'].max()\n",
    "df = df[df['k_expt'] < max_k]\n",
    "print(f\"Removed data point with highest k_expt = {max_k:.3f} W/mK\")\n",
    "print(f\"Dataset now has {len(df)} entries.\")\n",
    "\n",
    "\n",
    "df['k_expt'].hist(bins=15)\n",
    "plt.xlabel('Experimentally Measured Thermal Conductivity (W/mK)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Thermal Conductivity')\n",
    "plt.show()\n",
    "\n",
    "# composition-based features- ElementProperty\n",
    "\n",
    "df_comp = df.copy()\n",
    "df_comp['composition'] = df_comp['formula'].apply(lambda x: Composition(x) )\n",
    "df_comp.head()\n",
    "\n",
    "# featurization- ElementProperty\n",
    "\n",
    "el_prop_featuriser = ElementProperty.from_preset(preset_name='magpie', impute_nan=False)\n",
    "el_prop_featuriser.set_n_jobs(1)\n",
    "df_featurised = el_prop_featuriser.featurize_dataframe(df_comp, col_id='composition')\n",
    "\n",
    "print(df_featurised.shape)\n",
    "print(df_featurised.isnull().sum().sum())\n",
    "df_featurised.head()\n",
    "\n",
    "# feature engineering and cleaning\n",
    "\n",
    "y = df_featurised['k_expt']\n",
    "X_all = df_featurised.drop(columns=['k_expt', 'formula', 'k-units', 'k_condition', 'k_condition_units', 'composition'])\n",
    "\n",
    "print(\"Number of features before cleaning:\", X_all.shape[1])\n",
    "\n",
    "# identify columns with very small variance and drop them\n",
    "small_var_cols = X_all.columns[X_all.var() < 1e-5].tolist()\n",
    "print(\"Columns with very small variance:\", small_var_cols)\n",
    "X_all = X_all.drop(columns = small_var_cols)\n",
    "corr_matrix = X_all.corr(method='pearson')\n",
    "print(\"Number of features after removing small variance columns:\", X_all.shape[1])\n",
    "\n",
    "# remove highly correlated columns\n",
    "threshold = 0.99\n",
    "to_drop = set()\n",
    "for col in corr_matrix.columns:\n",
    "    high_corr = corr_matrix.index[(corr_matrix[col].abs() > threshold) & (corr_matrix.index != col)]\n",
    "    to_drop.update(high_corr)\n",
    "print(\"Columns to drop due to high correlation:\", to_drop)\n",
    "X = X_all.drop(columns=list(to_drop))\n",
    "print(\"Number of features after removing highly correlated columns:\", X.shape[1])\n",
    "\n",
    "#visualizing correlation matrix for remaining features\n",
    "corr_matrix = X.corr(method='pearson')\n",
    "plt.figure(figsize=(16,12))\n",
    "im = plt.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "plt.title('Pearson Correlation Matrix')\n",
    "plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns, rotation=90, fontsize=6)\n",
    "plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns, fontsize=6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# feature scaling and dataset splitting\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "test_fraction = 0.1\n",
    "validation_fraction = 0.2\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X_scaled, y,\n",
    "                                                          test_size=test_fraction,\n",
    "                                                          random_state=3)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval,\n",
    "                                                  test_size=validation_fraction/(1-test_fraction),\n",
    "                                                  random_state=3)\n",
    "print(f\"Training fraction: {X_train.shape[0] / X_scaled.shape[0]:.2f}\")\n",
    "print(f\"Validation fraction: {X_val.shape[0] / X_scaled.shape[0]:.2f}\")\n",
    "print(f\"Test fraction: {X_test.shape[0] / X_scaled.shape[0]:.2f}\")\n",
    "\n",
    "# dummy baseline model\n",
    "\n",
    "mean_train = y_train.mean()\n",
    "baseline_mae = mean_absolute_error(y_val, [mean_train] * len(y_val))\n",
    "print(f\"Baseline MAE (predicting mean thermal conductivity): {baseline_mae:.4f} W/mK\")\n",
    "\n",
    "# hyperparameter optimization\n",
    "\n",
    "n_estimators_list = [10, 50, 100, 500]\n",
    "train_maes = []\n",
    "val_maes = []\n",
    "\n",
    "for n in tqdm(n_estimators_list, desc='Training RF models'):\n",
    "    rf = RandomForestRegressor(n_estimators=n, random_state=3, n_jobs=1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_train_pred = rf.predict(X_train)\n",
    "    y_val_pred = rf.predict(X_val)\n",
    "    train_maes.append(mean_absolute_error(y_train, y_train_pred))\n",
    "    val_maes.append(mean_absolute_error(y_val, y_val_pred))\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(n_estimators_list, train_maes, marker='o', label='Train MAE')\n",
    "plt.plot(n_estimators_list, val_maes, marker='o', label='Validation MAE')\n",
    "plt.axhline(baseline_mae, color='gray', linestyle='--', label='Mean Baseline')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Random Forest: MAE vs. Number of Estimators')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# recursive feature elimination\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=500, random_state=3, n_jobs=1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Rank features by importance\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "importances_sorted = importances.sort_values(ascending=False)\n",
    "\n",
    "# Plot top 20 features by importance\n",
    "plt.figure(figsize=(8,6))\n",
    "importances_sorted.head(20).plot(kind='barh')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top 20 Most Important Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create dictionary to mimic RFE-style selection\n",
    "selected_features_dict = {}\n",
    "n_features_list = list(range(2, min(60, len(X.columns)), 2))  # up to 60 or total number of features\n",
    "val_errors = []\n",
    "train_errors = []\n",
    "\n",
    "for n_features in tqdm(n_features_list, desc='Feature Importance Selection Progress'):\n",
    "    selected = importances_sorted.head(n_features).index\n",
    "    selected_features_dict[n_features] = list(selected)\n",
    "    X_train_sel = X_train[:, [X.columns.get_loc(f) for f in selected]]\n",
    "    X_val_sel = X_val[:, [X.columns.get_loc(f) for f in selected]]\n",
    "    rf_temp = RandomForestRegressor(n_estimators=100, random_state=3, n_jobs=1)\n",
    "    rf_temp.fit(X_train_sel, y_train)\n",
    "    y_train_pred = rf_temp.predict(X_train_sel)\n",
    "    y_val_pred = rf_temp.predict(X_val_sel)\n",
    "    train_errors.append(mean_absolute_error(y_train, y_train_pred))\n",
    "    val_errors.append(mean_absolute_error(y_val, y_val_pred))\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(n_features_list, train_errors, label='Train MAE')\n",
    "plt.plot(n_features_list, val_errors, label='Validation MAE')\n",
    "plt.xlabel('Number of Selected Features')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Feature Importance Selection: MAE vs. Number of Features')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# final model evaluation\n",
    "\n",
    "final_features = selected_features_dict[10] \n",
    "rf_final = RandomForestRegressor(n_estimators=500, random_state=3, n_jobs=1)\n",
    "X_train_final = X_train[:, [X.columns.get_loc(f) for f in final_features]]\n",
    "rf_final.fit(X_train_final, y_train)\n",
    "\n",
    "# predict on train, validation, and test sets\n",
    "X_val_final = X_val[:, [X.columns.get_loc(f) for f in final_features]]\n",
    "X_test_final = X_test[:, [X.columns.get_loc(f) for f in final_features]]\n",
    "\n",
    "y_train_pred = rf_final.predict(X_train_final)\n",
    "y_val_pred = rf_final.predict(X_val_final)\n",
    "y_test_pred = rf_final.predict(X_test_final)\n",
    "\n",
    "# calculate metrics\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "mae_val = mean_absolute_error (y_val, y_val_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_val = r2_score(y_val, y_val_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharex=True, sharey=True)\n",
    "min_val = 0.0\n",
    "max_val = 450.0\n",
    "\n",
    "# train parity plot\n",
    "axes[0].scatter(y_train, y_train_pred, alpha=0.5, color='blue')\n",
    "axes[0].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)\n",
    "axes[0].set_title(f'Train (MAE={mae_train:.3f}, R2={r2_train:.3f})')\n",
    "axes[0].set_xlabel('True Formation Energy')\n",
    "axes[0].set_ylabel('Predicted Formation Energy')\n",
    "axes[0].set_aspect('equal', adjustable='box')\n",
    "\n",
    "# validation parity plot\n",
    "axes[1].scatter(y_val, y_val_pred, alpha=0.5, color='orange')\n",
    "axes[1].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)\n",
    "axes[1].set_title(f'Validation (MAE={mae_val:.3f}, R2={r2_val:.3f})')\n",
    "axes[1].set_xlabel('True Formation Energy')\n",
    "axes[1].set_ylabel('Predicted Formation Energy')\n",
    "axes[1].set_aspect('equal', adjustable='box')\n",
    "\n",
    "# test parity plot\n",
    "axes[2].scatter(y_test, y_test_pred, alpha=0.5, color='green')\n",
    "axes[2].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)\n",
    "axes[2].set_title(f'Test (MAE={mae_test:.3f}, R2={r2_test:.3f})')\n",
    "axes[2].set_xlabel('True Formation Energy')\n",
    "axes[2].set_ylabel('Predicted Formation Energy')\n",
    "axes[2].set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# featurization- ElementFraction\n",
    "\n",
    "el_frac_featuriser = ElementFraction()\n",
    "el_frac_featuriser.set_n_jobs(1)\n",
    "df_featurised = el_frac_featuriser.featurize_dataframe(df_comp, col_id='composition')\n",
    "\n",
    "print(df_featurised.shape)\n",
    "print(df_featurised.isnull().sum().sum())\n",
    "df_featurised.head()\n",
    "\n",
    "# feature engineering and cleaning\n",
    "\n",
    "y = df_featurised['k_expt']\n",
    "X_all = df_featurised.drop(columns=['k_expt', 'formula', 'k-units', 'k_condition', 'k_condition_units', 'composition'])\n",
    "\n",
    "print(\"Number of features before cleaning:\", X_all.shape[1])\n",
    "\n",
    "# identify columns with very small variance and drop them\n",
    "small_var_cols = X_all.columns[X_all.var() < 1e-5].tolist()\n",
    "print(\"Columns with very small variance:\", small_var_cols)\n",
    "X_all = X_all.drop(columns = small_var_cols)\n",
    "corr_matrix = X_all.corr(method='pearson')\n",
    "print(\"Number of features after removing small variance columns:\", X_all.shape[1])\n",
    "\n",
    "# remove highly correlated columns\n",
    "threshold = 0.99\n",
    "to_drop = set()\n",
    "for col in corr_matrix.columns:\n",
    "    high_corr = corr_matrix.index[(corr_matrix[col].abs() > threshold) & (corr_matrix.index != col)]\n",
    "    to_drop.update(high_corr)\n",
    "print(\"Columns to drop due to high correlation:\", to_drop)\n",
    "X = X_all.drop(columns=list(to_drop))\n",
    "print(\"Number of features after removing highly correlated columns:\", X.shape[1])\n",
    "\n",
    "#visualizing correlation matrix for remaining features\n",
    "corr_matrix = X.corr(method='pearson')\n",
    "plt.figure(figsize=(16,12))\n",
    "im = plt.imshow(corr_matrix, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.colorbar(im, fraction=0.046, pad=0.04)\n",
    "plt.title('Pearson Correlation Matrix')\n",
    "plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns, rotation=90, fontsize=6)\n",
    "plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns, fontsize=6)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# feature scaling and dataset splitting\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "test_fraction = 0.1\n",
    "validation_fraction = 0.2\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X_scaled, y,\n",
    "                                                          test_size=test_fraction,\n",
    "                                                          random_state=3)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval,\n",
    "                                                  test_size=validation_fraction/(1-test_fraction),\n",
    "                                                  random_state=3)\n",
    "print(f\"Training fraction: {X_train.shape[0] / X_scaled.shape[0]:.2f}\")\n",
    "print(f\"Validation fraction: {X_val.shape[0] / X_scaled.shape[0]:.2f}\")\n",
    "print(f\"Test fraction: {X_test.shape[0] / X_scaled.shape[0]:.2f}\")\n",
    "\n",
    "# dummy baseline model\n",
    "\n",
    "mean_train = y_train.mean()\n",
    "baseline_mae = mean_absolute_error(y_val, [mean_train] * len(y_val))\n",
    "print(f\"Baseline MAE (predicting mean thermal conductivity): {baseline_mae:.4f} W/mK\")\n",
    "\n",
    "# hyperparameter optimization\n",
    "\n",
    "n_estimators_list = [10, 50, 100, 500]\n",
    "train_maes = []\n",
    "val_maes = []\n",
    "\n",
    "for n in tqdm(n_estimators_list, desc='Training RF models'):\n",
    "    rf = RandomForestRegressor(n_estimators=n, random_state=3, n_jobs=1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_train_pred = rf.predict(X_train)\n",
    "    y_val_pred = rf.predict(X_val)\n",
    "    train_maes.append(mean_absolute_error(y_train, y_train_pred))\n",
    "    val_maes.append(mean_absolute_error(y_val, y_val_pred))\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(n_estimators_list, train_maes, marker='o', label='Train MAE')\n",
    "plt.plot(n_estimators_list, val_maes, marker='o', label='Validation MAE')\n",
    "plt.axhline(baseline_mae, color='gray', linestyle='--', label='Mean Baseline')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Random Forest: MAE vs. Number of Estimators')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# recursive feature elimination\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=3, n_jobs=1)\n",
    "n_features_list = list(range(5, X_val.shape[1]+1, 5))\n",
    "val_errors = []\n",
    "train_errors = []\n",
    "selected_features_dict = {}\n",
    "\n",
    "for n_features in tqdm(n_features_list, desc='RFE Progress'):\n",
    "    rfe = RFE(estimator=rf, n_features_to_select=n_features, step=5)\n",
    "    rfe.fit(X_train, y_train)\n",
    "    selected_features_dict[n_features] = list(X.columns[rfe.support_])\n",
    "    X_train_rfe = rfe.transform(X_train)\n",
    "    rf.fit(X_train_rfe, y_train)\n",
    "    y_train_pred = rf.predict(X_train_rfe)\n",
    "    train_errors.append(mean_absolute_error(y_train, y_train_pred))\n",
    "    X_val_rfe = rfe.transform(X_val)\n",
    "    y_val_pred = rf.predict(X_val_rfe)\n",
    "    val_errors.append(mean_absolute_error(y_val, y_val_pred))\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(n_features_list, train_errors, label='Train MAE')\n",
    "plt.plot(n_features_list, val_errors, label='Validation MAE')\n",
    "plt.xlabel('Number of Selected Features')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('RFE: MAE vs. Number of Features')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# final model evaluation\n",
    "\n",
    "final_features = selected_features_dict[20] \n",
    "rf_final = RandomForestRegressor(n_estimators=100, random_state=3, n_jobs=1)\n",
    "X_train_final = X_train[:, [X.columns.get_loc(f) for f in final_features]]\n",
    "rf_final.fit(X_train_final, y_train)\n",
    "\n",
    "# predict on train, validation, and test sets\n",
    "X_val_final = X_val[:, [X.columns.get_loc(f) for f in final_features]]\n",
    "X_test_final = X_test[:, [X.columns.get_loc(f) for f in final_features]]\n",
    "\n",
    "y_train_pred = rf_final.predict(X_train_final)\n",
    "y_val_pred = rf_final.predict(X_val_final)\n",
    "y_test_pred = rf_final.predict(X_test_final)\n",
    "\n",
    "# calculate metrics\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "mae_val = mean_absolute_error (y_val, y_val_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_val = r2_score(y_val, y_val_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharex=True, sharey=True)\n",
    "min_val = 0.0\n",
    "max_val = 450.0\n",
    "\n",
    "# train parity plot\n",
    "axes[0].scatter(y_train, y_train_pred, alpha=0.5, color='blue')\n",
    "axes[0].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)\n",
    "axes[0].set_title(f'Train (MAE={mae_train:.3f}, R2={r2_train:.3f})')\n",
    "axes[0].set_xlabel('True Formation Energy')\n",
    "axes[0].set_ylabel('Predicted Formation Energy')\n",
    "axes[0].set_aspect('equal', adjustable='box')\n",
    "\n",
    "# validation parity plot\n",
    "axes[1].scatter(y_val, y_val_pred, alpha=0.5, color='orange')\n",
    "axes[1].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)\n",
    "axes[1].set_title(f'Validation (MAE={mae_val:.3f}, R2={r2_val:.3f})')\n",
    "axes[1].set_xlabel('True Formation Energy')\n",
    "axes[1].set_ylabel('Predicted Formation Energy')\n",
    "axes[1].set_aspect('equal', adjustable='box')\n",
    "\n",
    "# test parity plot\n",
    "axes[2].scatter(y_test, y_test_pred, alpha=0.5, color='green')\n",
    "axes[2].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)\n",
    "axes[2].set_title(f'Test (MAE={mae_test:.3f}, R2={r2_test:.3f})')\n",
    "axes[2].set_xlabel('True Formation Energy')\n",
    "axes[2].set_ylabel('Predicted Formation Energy')\n",
    "axes[2].set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a58cac9",
   "metadata": {},
   "source": [
    "### Assignment 2\n",
    "\n",
    "Go through an entire ML pipeline again, but this time use the Materials Project API to query a dataset that contains around 1-3k data points. Make the query more and more selective until only a few thousand entries are returned (e.g., by limiting the number of unique elements in the unit cell, limiting the band gap range, limiting the energy above hull value, etc.). Select one of the many possible target properties that the Materials Project has to offer (see homework 3, assignment 2, on how to get the information of all possible fields that can be requested and/or queried). \n",
    "\n",
    "Featurize the dataset you queried with a structural featurizer from `DScribe` and go through the remaining ML pipeline (similar to assignment 1). Summarize the performance metrics on the test set and upload the parity plots as a PNG files to the repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03142f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mp_api.client import MPRester\n",
    "import pandas as pd\n",
    "from dscribe.descriptors import SOAP\n",
    "from pymatgen.io.ase import AseAtomsAdaptor\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "api_key = 'ncNeAsZOmf7KOxBjJcP4O8O1NBrWciqx'\n",
    "\n",
    "with MPRester(api_key) as mpr:\n",
    "    data = data = mpr.materials.summary.search(theoretical=False, nelements= (4, 9))\n",
    "    print(f'Size of dataset for materials containing 4-9 unique elements within unit cell: {len(data)}')\n",
    "    data = data = mpr.materials.summary.search(theoretical=False, nelements= (5, 9))\n",
    "    print(f'Size of dataset for materials containing 5-9 unique elements within unit cell: {len(data)}')\n",
    "    data = mpr.materials.summary.search(theoretical=False, num_elements=(6, 9),\n",
    "                                        fields=['material_id', 'band_gap', 'formula_pretty', 'structure'])\n",
    "    print(f'Size of dataset for materials containing 6-9 unique elements within unit cell: {len(data)}')\n",
    "\n",
    "df = pd.DataFrame([\n",
    "    {\"material_id\": d.material_id,\n",
    "     \"formula\": d.formula_pretty,\n",
    "     \"band_gap\": d.band_gap,\n",
    "     \"structure\": d.structure}\n",
    "     for d in data\n",
    "])\n",
    "\n",
    "df.head()\n",
    "\n",
    "species = list({str(sp) for s in df[\"structure\"] for sp in s.symbol_set})\n",
    "soap = SOAP(\n",
    "    species=species,\n",
    "    r_cut=5.0,\n",
    "    n_max=4,\n",
    "    l_max=3,\n",
    "    average=\"inner\",\n",
    "    sparse=False\n",
    ")\n",
    "\n",
    "ase_adaptor = AseAtomsAdaptor()\n",
    "features = []\n",
    "\n",
    "for s in tqdm(df[\"structure\"], desc=\"Featurizing structures\"):\n",
    "    atoms = ase_adaptor.get_atoms(s)\n",
    "    feat = soap.create(atoms)\n",
    "    features.append(feat)\n",
    "\n",
    "X = np.array(features)\n",
    "y = df[\"band_gap\"].values\n",
    "print(f\"Feature shape: {X.shape}\")\n",
    "\n",
    "# feature scaling and dataset splitting\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "test_fraction = 0.1\n",
    "validation_fraction = 0.2\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X_scaled, y,\n",
    "                                                          test_size=test_fraction,\n",
    "                                                          random_state=3)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval,\n",
    "                                                  test_size=validation_fraction/(1-test_fraction),\n",
    "                                                  random_state=3)\n",
    "print(f\"Training fraction: {X_train.shape[0] / X_scaled.shape[0]:.2f}\")\n",
    "print(f\"Validation fraction: {X_val.shape[0] / X_scaled.shape[0]:.2f}\")\n",
    "print(f\"Test fraction: {X_test.shape[0] / X_scaled.shape[0]:.2f}\")\n",
    "\n",
    "# dummy baseline model\n",
    "\n",
    "mean_train = y_train.mean()\n",
    "baseline_mae = mean_absolute_error(y_val, [mean_train] * len(y_val))\n",
    "print(f\"Baseline MAE (predicting mean band gap): {baseline_mae:.4f} eV\")\n",
    "\n",
    "# hyperparameter optimization\n",
    "\n",
    "n_estimators_list = [10, 50, 100, 500]\n",
    "train_maes = []\n",
    "val_maes = []\n",
    "\n",
    "for n in tqdm(n_estimators_list, desc='Training RF models'):\n",
    "    rf = RandomForestRegressor(n_estimators=n, random_state=3, n_jobs=1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_train_pred = rf.predict(X_train)\n",
    "    y_val_pred = rf.predict(X_val)\n",
    "    train_maes.append(mean_absolute_error(y_train, y_train_pred))\n",
    "    val_maes.append(mean_absolute_error(y_val, y_val_pred))\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(n_estimators_list, train_maes, marker='o', label='Train MAE')\n",
    "plt.plot(n_estimators_list, val_maes, marker='o', label='Validation MAE')\n",
    "plt.axhline(baseline_mae, color='gray', linestyle='--', label='Mean Baseline')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Random Forest: MAE vs. Number of Estimators')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# feature selection using random forest importances\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=3, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# rank features by importance\n",
    "importances = rf.feature_importances_\n",
    "sorted_idx = np.argsort(importances)[::-1]\n",
    "\n",
    "n_features_list = [5, 10, 25, 50, 100, 200, 400]  \n",
    "train_errors, val_errors = [], []\n",
    "\n",
    "for n_features in tqdm(n_features_list, desc=\"Evaluating top features\"):\n",
    "    top_idx = sorted_idx[:n_features]\n",
    "    rf.fit(X_train[:, top_idx], y_train)\n",
    "    y_train_pred = rf.predict(X_train[:, top_idx])\n",
    "    y_val_pred = rf.predict(X_val[:, top_idx])\n",
    "    train_errors.append(mean_absolute_error(y_train, y_train_pred))\n",
    "    val_errors.append(mean_absolute_error(y_val, y_val_pred))\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(n_features_list, train_errors, marker='o', label='Train MAE')\n",
    "plt.plot(n_features_list, val_errors, marker='o', label='Validation MAE')\n",
    "plt.axhline(baseline_mae, color='gray', linestyle='--', label='Mean Baseline')\n",
    "plt.xlabel('Number of Top Features Used')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('Random Forest: MAE vs. Number of Top Features')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# final model evaluation\n",
    "\n",
    "# pick the number of top features with lowest validation MAE\n",
    "best_n_idx = np.argmin(val_errors)\n",
    "best_n_features = n_features_list[best_n_idx]\n",
    "print(f\"Best number of features based on validation MAE: {best_n_features}\")\n",
    "\n",
    "top_idx = sorted_idx[:best_n_features]\n",
    "\n",
    "rf_final = RandomForestRegressor(n_estimators=100, random_state=3, n_jobs=-1)\n",
    "rf_final.fit(X_train[:, top_idx], y_train)\n",
    "\n",
    "y_train_pred = rf_final.predict(X_train[:, top_idx])\n",
    "y_val_pred = rf_final.predict(X_val[:, top_idx])\n",
    "y_test_pred = rf_final.predict(X_test[:, top_idx])\n",
    "\n",
    "mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "mae_val = mean_absolute_error(y_val, y_val_pred)\n",
    "mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "\n",
    "r2_train = r2_score(y_train, y_train_pred)\n",
    "r2_val = r2_score(y_val, y_val_pred)\n",
    "r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# parity plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharex=True, sharey=True)\n",
    "min_val, max_val = 0.0, 5.0\n",
    "\n",
    "# train parity plot\n",
    "axes[0].scatter(y_train, y_train_pred, alpha=0.5, color='blue')\n",
    "axes[0].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)\n",
    "axes[0].set_title(f'Train (MAE={mae_train:.3f}, R2={r2_train:.3f})')\n",
    "axes[0].set_xlabel('True Band Gap (eV)')\n",
    "axes[0].set_ylabel('Predicted Band Gap (eV)')\n",
    "axes[0].set_aspect('equal', adjustable='box')\n",
    "\n",
    "# validation parity plot\n",
    "axes[1].scatter(y_val, y_val_pred, alpha=0.5, color='orange')\n",
    "axes[1].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)\n",
    "axes[1].set_title(f'Validation (MAE={mae_val:.3f}, R2={r2_val:.3f})')\n",
    "axes[1].set_xlabel('True Band Gap (eV)')\n",
    "axes[1].set_ylabel('Predicted Band Gap (eV)')\n",
    "axes[1].set_aspect('equal', adjustable='box')\n",
    "\n",
    "# test parity plot\n",
    "axes[2].scatter(y_test, y_test_pred, alpha=0.5, color='green')\n",
    "axes[2].plot([min_val, max_val], [min_val, max_val], 'k--', lw=2)\n",
    "axes[2].set_title(f'Test (MAE={mae_test:.3f}, R2={r2_test:.3f})')\n",
    "axes[2].set_xlabel('True Band Gap (eV)')\n",
    "axes[2].set_ylabel('Predicted Band Gap (eV)')\n",
    "axes[2].set_aspect('equal', adjustable='box')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a968891",
   "metadata": {},
   "source": [
    "### Assignment 3\n",
    "\n",
    "Please, go through the first 6 pages of this paper: `Resources/Papers/Musil_Chemical_Review_2021_Review-of-Descriptors.pdf`. We discussed these aspects during the lectures, but please summarize any key insights that you may have missed during the lecture but understand better after reading the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ee7dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paper is driven by a need to transform Cartesian coordinates of atoms into a suitable representation\n",
    "# Descriptor/fingerprint are used interchangeably to indicate properties that are easier to compute than what you actually want to predict, but are strongly correlated to them\n",
    "# SOAP is an example of a representation- precisely characterizes the instantaneous arrangement of atoms\n",
    "    # Applies geometric and algebraic manipulations to Cartesian coordinates\n",
    "    # Fulfills smoothness and symmetry with respect to isometries\n",
    "# Space of features takes the form of vector space\n",
    "    # May be simpler/more natural to describe the relationship between pairs of configurations\n",
    "# Representations can be used with little modification for any atomistic system\n",
    "# QSPR is quantitative structure-property relationship\n",
    "    # This can be compared to bottom-up modeling, these are different from machine-learned potentials\n",
    "# ML tasks can benefit from being based on local features, rather than global\n",
    "# There are many trivial symmetries- this can lead to inefficiency\n",
    "    # A Z-matrix is a collection of internal coordinates that is sufficient to fully characterize structure geometry\n",
    "        # Has limitations- bonding patterns will change in the simulation of a chemically active system\n",
    "# Majority of atomic-scale properties are continuous, smooth functions of atomic coordinates\n",
    "    # Features and internal coordinates are usually smooth\n",
    "    # The Coulomb matrix helps to obtain permutation invariance\n",
    "# We can assume an additive decomposition of properties to make problems simpler"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4msd-teacher",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
